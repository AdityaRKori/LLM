{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.46.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (2.32.5)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (0.7.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from transformers==4.46.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.46.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.46.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.46.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from requests->transformers==4.46.2) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.46.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.1+cu121\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (798.9 MB)\n",
      "Collecting torchvision==0.19.1+cu121\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.1 MB)\n",
      "Collecting torchaudio==2.4.1+cu121\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.45.2 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.45.2\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "  torch==2.4.1+cu121 \\\n",
    "  torchvision==0.19.1+cu121 \\\n",
    "  torchaudio==2.4.1+cu121 \\\n",
    "  --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "  transformers==4.45.2 \\\n",
    "  accelerate==0.34.2 \\\n",
    "  tokenizers==0.20.1 \\\n",
    "  huggingface-hub==0.25.2 \\\n",
    "  safetensors \\\n",
    "  numpy==1.26.4 \\\n",
    "  pandas==2.2.3 \\\n",
    "  pyarrow==17.0.0 \\\n",
    "  matplotlib==3.8.4 \\\n",
    "  scikit-learn==1.5.2 \\\n",
    "  datasets==3.0.2 \\\n",
    "  langchain==0.2.16 \\\n",
    "  langchain-community==0.2.16 \\\n",
    "  langchain-core==0.2.38\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edeae6ca2ac4e4eb0324ea872867f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=False,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Artificial Intelligence (AI) in healthcare refers to the use of algorithms and software to approximate human cognition in the analysis, interpretation, and comprehension of complex medical and healthcare data. The primary aim of AI in healthcare is to enhance the quality of care, improve patient outcomes, and reduce healthcare costs.\n",
      "\n",
      "AI technologies are being used in various aspects of healthcare, including:\n",
      "\n",
      "1. Diagnosis and treatment: AI algorithms can analyze medical images, such as X-rays, CT scans, and MRIs, to detect diseases and abnormalities with high accuracy. AI can also help in predicting the risk of diseases, such as diabetes, heart disease, and cancer, based on patient data.\n",
      "\n",
      "2. Drug discovery and development: AI can help in identifying potential drug candidates by analyzing large datasets of chemical compounds and biological data. A\n"
     ]
    }
   ],
   "source": [
    "response = generator(\n",
    "    \"Explain AI in Healthcare\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the bustling city of Mumbai, where the air is thick with the aroma of spices and the sound of honking cars, a revolution was quietly unfolding within the walls of the Aarogya Hospital. Dr. Anika Mehta, a renowned cardiologist, was on the verge of integrating a groundbreaking AI system into her practice, a system that promised to transform the landscape of Indian healthcare.\n",
      "\n",
      "The AI, named \"Aarav,\" was the brainchild of a team of Indian engineers and medical professionals who had seen the potential of artificial intelligence in diagnosing and treating patients. Aarav was not just any AI; it\n"
     ]
    }
   ],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"write story of AI in indian healthcare\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(\n",
    "    messages,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.0,)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why do programmers always mix up Halloween and Christmas?\n",
      "\n",
      "\n",
      "Because Oct 31 equals Dec 25!\n"
     ]
    }
   ],
   "source": [
    "# The prompt (user input / query)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"you are a sarcastic comedy writer\"},\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke about programmers\"}\n",
    "]\n",
    "\n",
    "# Generate output\n",
    "output = generator(\n",
    "    messages,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.0,)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to gently roast anything the user asks for. Keep it funny, not cruel. Give me a joke about AI that behaves like a grumpy old man. Why did the AI refuse to play cards with the grumpy old man? Because every time it calculated the odds, it realized the old man's hand was a straight flush, and it didn't want to deal with the high stakes of his mood!\n"
     ]
    }
   ],
   "source": [
    "# 1. Your messages\n",
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"Your job is to gently roast anything the user asks for. Keep it funny, not cruel.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a joke about AI that behaves like a grumpy old man.\"}\n",
    "]\n",
    "\n",
    "# 2. Create the chat prompt\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 3. Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 4. Generate\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# 5. Decode\n",
    "final_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(final_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Email\n",
      "Subject: Heartfelt Apologies for the Garden Incident\n",
      "\n",
      "\n",
      "Dear Sarah,\n",
      "\n",
      "\n",
      "I hope this message finds you well\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
    " \n",
    "# Tokenize the input prompt, requesting both input_ids and attention_mask\n",
    "tokenized_output = tokenizer(prompt, return_tensors=\"pt\")\n",
    " \n",
    "input_ids = tokenized_output.input_ids.to(model.device) # Use model.device for safety\n",
    "attention_mask = tokenized_output.attention_mask.to(model.device)\n",
    " \n",
    " \n",
    "# 2. Generate the text (Crucially, increasing max_new_tokens)\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask, # Pass the attention mask\n",
    "    max_new_tokens=30,             # *** Increased to 256 for a full email ***\n",
    "    do_sample=True,                 # Recommended for creative tasks\n",
    "    temperature=0.7                 # Recommended for creative tasks\n",
    ")\n",
    " \n",
    "# 3. Decode and Clean the Output\n",
    "full_output = tokenizer.decode(generation_output[0], skip_special_tokens=False)\n",
    " \n",
    "# Get the length of the original prompt's input tokens\n",
    "prompt_length = input_ids.shape[-1]\n",
    " \n",
    "# Slice the decoded string to get only the generated part\n",
    "# (Start from the token immediately after the input prompt)\n",
    "generated_text = tokenizer.decode(generation_output[0][prompt_length:], skip_special_tokens=True).strip()\n",
    " \n",
    "print(\"--- Generated Email\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
      "           293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
      "           372,  9559, 29889, 32001]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
